{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6e941-ea3a-4763-960d-c48c3ea9ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import pandas as pd\n",
    "import re, glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f1a63-4cc8-4a21-93aa-bdda9eda6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful constants, update once groupthink works\n",
    "tasks = [\"interpolation\", \"extrapolation\"]\n",
    "models = [\"pondernet\"]\n",
    "elems = {\"interpolation\": 16, \"extrapolation\": 24}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ebc58-047b-4970-b88c-bc6a04198c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(path):\n",
    "    \"\"\"Fetches all relevant paths\"\"\"\n",
    "    files = []\n",
    "    for f in glob.glob(path):\n",
    "        files.append(f)\n",
    "    return files\n",
    "\n",
    "def parse_tensorboard(path, scalars):\n",
    "    \"\"\"returns a dictionary of pandas dataframes for each requested scalar\"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        path,\n",
    "        size_guidance={event_accumulator.SCALARS: 0},\n",
    "    )\n",
    "    _absorb_print = ea.Reload()\n",
    "    # make sure the scalars are in the event accumulator tags\n",
    "    if not all(\n",
    "        s in ea.Tags()[\"scalars\"] for s in scalars\n",
    "    ):\n",
    "        return None\n",
    "    assert all(\n",
    "        s in ea.Tags()[\"scalars\"] for s in scalars\n",
    "    ), f\"some scalars were not found in the event accumulator, {scalars}\"\n",
    "    return {k: pd.DataFrame(ea.Scalars(k)) for k in scalars}\n",
    "\n",
    "def get_df(paths, scalars):\n",
    "    \"\"\"Gets all dfs of all logs files according to the desired scalars and combines them.\n",
    "    Only use this if you have single values per scalar per log file, not for time series.\"\"\"\n",
    "    df_list = {s:[] for s in scalars}\n",
    "    for f in paths:\n",
    "        dict_df = parse_tensorboard(f, scalars)\n",
    "        if not dict_df:\n",
    "            print(f\"Not all requested metrics found in {f}\")\n",
    "            continue\n",
    "        for key, val in dict_df.items():\n",
    "            val[\"file\"] = f\n",
    "            val.rename(columns={'value': key}, inplace=True)\n",
    "            val = val.set_index(val[\"file\"])\n",
    "            df_list[key].append(val[[\"step\", key]])\n",
    "    dict_result = {s : pd.concat(df_list[s]) for s in scalars}\n",
    "    df = pd.concat(list(dict_result.values()), axis = 1)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_timeseries_df(paths, scalars):\n",
    "    \"\"\"Gets all dfs of all log files and merges them according to similar step.\"\"\"\n",
    "    df_list = {s:[] for s in scalars}\n",
    "    for i, f in enumerate(paths):\n",
    "        dict_df = parse_tensorboard(f, scalars)\n",
    "        if not dict_df:\n",
    "            print(f\"Not all requested metrics found in {f}\")\n",
    "            continue\n",
    "        for key, val in dict_df.items():\n",
    "            val.rename(columns={'value': key+f\"-v{i}\"}, inplace=True)\n",
    "            val.set_index(val[\"step\"], inplace=True)\n",
    "            df_list[key].append(val.drop([\"wall_time\"], axis=1))\n",
    "    dict_result = {s : (pd.concat(df_list[s], axis=1)).drop([\"step\"], axis=1) for s in scalars}\n",
    "    for s in scalars:\n",
    "        dict_result[s][\"Average\"] = dict_result[s].mean(axis = 1)\n",
    "    return dict_result\n",
    "\n",
    "def get_metrics(phase, scalars):\n",
    "    \"\"\"Gets metrics\"\"\"\n",
    "    dfs = {}\n",
    "    for m in models:\n",
    "        dfs[m] = {}\n",
    "        for t in tasks:\n",
    "            num_elems = elems[t]\n",
    "            dir_path = f\"../models/{m}_{t}_{elems[t]}/*/*events*\"\n",
    "            if phase == \"test\":\n",
    "                # dir_path = f\"../models/{m}_{t}_{elems[t]}/*/*events*Matteo*\"\n",
    "                files = get_paths(dir_path)\n",
    "                df = get_df(files, scalars)\n",
    "                dfs[m][t] = df.drop([\"step\"], axis=1)\n",
    "            elif phase == \"val\":\n",
    "                # dir_path = f\"../models/{m}_{t}_{elems[t]}/*/*events*lisa*\"\n",
    "                files = get_paths(dir_path)\n",
    "                dfs[m][t] = get_timeseries_df(files, scalars) \n",
    "    if phase == \"test\":\n",
    "        to_graph = {t:{s:{m:dfs[m][t][[s]] for m in models} for s in scalars} for t in tasks}\n",
    "    else:\n",
    "        to_graph = {t:{s:{m:dfs[m][t][s] for m in models} for s in scalars} for t in tasks}\n",
    "    return to_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f7ea0-3ef3-4a69-b8ca-ceae62c74172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting scalars at test for comparison across models\n",
    "phase = \"test\"\n",
    "scalars = [f'{phase}/accuracy_halted_step', f'{phase}/halting_step']\n",
    "to_graph = get_metrics(phase, scalars)\n",
    "f, axs = plt.subplots(2, 2, dpi=300, figsize=(7, 5), sharey=\"row\", sharex=\"col\")\n",
    "for i, t in enumerate(tasks):\n",
    "    for j, s in enumerate(scalars):\n",
    "        for m in models:\n",
    "            to_graph[t][s][m].rename(columns={s: m.title()}, inplace=True)\n",
    "            to_graph[t][s][m].boxplot(ax=axs[j,i], grid = False)\n",
    "            if i % 2 == 0:\n",
    "                if j % 2 == 0:\n",
    "                    axs[j,i].set_ylabel(\"Accuracy\")\n",
    "                else:\n",
    "                    axs[j,i].set_ylabel(\"Halting Step\")\n",
    "    axs[0,i].set_title(t.title())\n",
    "plt.suptitle('Model Results')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388027d-fa45-47db-8eac-0fae77f0a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting time series of training for comparison across models\n",
    "phase = \"val\"\n",
    "scalars = [f'{phase}/accuracy_halted_step', f'{phase}/halting_step']\n",
    "to_graph = get_metrics(phase, scalars)\n",
    "f, axs = plt.subplots(2, 2, dpi=300, figsize=(12, 8), sharey=\"row\", sharex=\"col\")\n",
    "for i, t in enumerate(tasks):\n",
    "    for j, s in enumerate(scalars):\n",
    "        for m in models:\n",
    "            to_graph[t][s][m].rename(columns={\"Average\": m.title()+\" Average\"}, inplace=True)\n",
    "            to_graph[t][s][m][m.title()+\" Average\"].plot(ax=axs[j,i], grid = False, style=[\"-\"])\n",
    "            if i % 2 == 0:\n",
    "                if j % 2 == 0:\n",
    "                    axs[j,i].set_ylabel(\"Accuracy\")\n",
    "                else:\n",
    "                    axs[j,i].set_ylabel(\"Halting Step\")\n",
    "    axs[0,i].set_title(t.title())\n",
    "    axs[1,i].set_xlabel(\"Compute Step\")\n",
    "plt.suptitle('Model Validation Results')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
